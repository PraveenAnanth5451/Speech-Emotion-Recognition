# ğŸ¤ Speech Emotion Recognition (SER)

A machine learning project that detects human emotions from speech using **audio preprocessing** and **deep learning (TensorFlow)**.
The system records or loads an audio file, extracts features using **Librosa**, and predicts the emotion using a trained model.
A simple **Kivy UI** is included to interactively record, process, and display the predicted emotion.

---

## ğŸ“Œ Features

* ğŸ™ **Record live audio** using `sounddevice`
* ğŸµ **Extract MFCCs and audio features** using `librosa`
* ğŸ§  **Deep learning model (TensorFlow) for emotion classification**
* ğŸ“Š Data processing, visualization, and training notebooks
* ğŸ–¥ **Kivy-based UI** for user interaction
* ğŸ’¾ Save & load trained models using `joblib` or `.h5`
* ğŸ”Š Works on WAV audio files

---

## ğŸ“ Project Structure

Matches your actual folder structure:

```
speech-emotion-recognition/
â”‚
â”œâ”€â”€ app/                  # Kivy app code/UI
â”œâ”€â”€ data/                 # Audio dataset
â”œâ”€â”€ model/                # Saved trained models (.h5, .pkl)
â”œâ”€â”€ notebooks/            # Jupyter notebooks for training & experiments
â”œâ”€â”€ venv/                 # Virtual environment (ignored in git)
â”‚
â”œâ”€â”€ recorded_audio.wav    # Example recorded audio
â”œâ”€â”€ temp.wav              # Temporary recording file
â”œâ”€â”€ test_kivy.py          # Kivy test script
â”œâ”€â”€ summa.ipynb           # Notebook for experiments
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## ğŸ›  Technologies Used

* **Kivy** â€“ User Interface
* **TensorFlow / Keras** â€“ Deep learning model
* **Librosa** â€“ Audio feature extraction
* **NumPy / Pandas** â€“ Data handling
* **scikit-learn** â€“ Feature scaling + classical models
* **joblib** â€“ Saving ML models
* **sounddevice** â€“ Recording audio
* **matplotlib / seaborn** â€“ Visualization

---

## ğŸ™ How It Works

1. User records audio or selects a WAV file
2. Audio is processed using Librosa
3. MFCCs + other features are extracted
4. Features are fed into the trained TensorFlow model
5. The model outputs a predicted emotion (e.g., Happy, Angry, Neutral)
6. Kivy app displays the result

---

## ğŸ“Š Model

The model is a deep learning classifier trained on MFCC features.
Typical architecture:

* Dense Layers
* Dropout
* Softmax output (multi-class emotion prediction)

---

## ğŸ˜Š Supported Emotions

(Depends on your dataset, example:)

* Happy
* Sad
* Angry
* Neutral
* Fear
* Surprise

---

## ğŸ”® Future Improvements

* Add mobile APK using **Kivy + Buildozer**
* Use CNN or LSTM models for better accuracy
* Add real-time continuous emotion tracking
* Deploy as a web app

---
